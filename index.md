---
layout: base.njk
title: LUCIA OS
eleventyNavigation:
  key: Home
  order: 1
---


The Human Cost of Computing
Our capacity to adapt is truly something to admire. Think about the journey: from hunter-gatherers to modern techno-sapiens, we've navigated profound, irreversible shifts in how we act and think. In this long timeline, computers undeniably contributed the most recent transformation. And when I say computers, I mean the entire, sprawling ecosystem, encompassing not just desktops and laptops, but the smartphones, tablets, watches, smart speakers, even thermostats that permeate our daily existence. What binds them together is their collective power to reshape our cognition, our actions, and our habits, often in ways that aren't purely beneficial.

Today, countless talented designers and engineers work tirelessly, aiming to make our digital interactions feel seamless, almost natural. But they face an uphill battle, constrained by the weight of legacy technologies. Trying to introduce something fundamentally different encounters immense friction. A world without the familiar structure of files and folders feels deeply unsettling to most. Suggest altering the standard keyboard, a design, rooted in mechanical limitations of early typewriters, and you'll likely spark public outrage. These reactions starkly reveal how deeply we've internalized conventions that, while perhaps necessary once, were never truly natural to begin with.

The result is that our current digital landscape often feels like a patchwork quilt. It stitches together foundational concepts born in the 1960s, layering them with the polished visual gloss of the 2020s. We celebrate faster chips, yet this raw speed hasn't automatically translated into genuinely smarter, more intuitive experiences. We've merely engineered abstractions upon abstractions, building ever higher without pausing to reconsider the shaky foundations below.

Consider the pillars of modern computing. Files and folders are digital ghosts, artifacts echoing the physical limitations of early hierarchical file systems. Applications are relics of outdated software distribution models, not necessarily the best way to organize capability. The mouse and keyboard remain interfaces optimized for the static world of office furniture, not the fluid, dynamic range of human expression.

These aren't just neutral tools. They are inherited constraints, shaping how we think and interact in ways dictated by a past we've failed to fully transcend.

The Shape of Thought
Most people underestimate how radically better human-centered computing could be, just as they underestimate how deeply entrenched our current condition truly is.

Let's pretend, just for a moment, that a computer is a person.

Our interactions with people range wildly, from rich, long-form, multi-modal exchanges to brief, constrained text messages. Communication bandwidth between people who know each other well is often far greater than between complete strangers. When you share history with someone, the immediate bandwidth expands dramatically. Incoming information grounds itself within a vast environment of shared context, common sense, and unspoken cues. That deep, context-rich connection is the kind of relationship we desperately need to achieve between a human and a machine.

As humans, we think in stories, emotions, and context. Our minds don't neatly file memories alphabetically or chronologically. We remember through webs of relationships, by places and people, by purpose and feeling. Crucially, each of us remembers differently. There is tremendous breadth in how people think, how they learn, and how they communicate. Yet our digital systems relentlessly force us into rigid boxes, like folders, apps, and icons. They demand significant cognitive overhead for even the simplest tasks. This cognitive overhead slowly calcifies into habits, and eventually, these habits overtake our natural ways of being. Personal computers are hardly personal.

To achieve truly human-centered computing, we must first deeply understand what makes human interactions, both with each other and with the world, possible and meaningful. We must step back. Not just a few paces, but far enough to revisit the dawn of computing, armed with the knowledge we have now. Then, guided by these fundamental principles of human interaction, we can begin to build the next generation of form factors, computing hardware, and operating systems from the ground up.

Seeds of Cognition
AI assistants bolted onto a conventional OS certainly have their place. However, they're once again constrained by the underlying technology which defines information access and representation primarily by dividing it into documents and applications.

What we truly need is an operating system that is built differently from the start. By exploring principles of human cognition, we can build a foundation that will feel much more natural for human-computer interaction. A cognitive computer.

At the core of this new architecture is the concept of a kernel. While traditional computers have many modules considered foundational, at the lowest level, there is typically a Von Neumann machine reading ones and zeros and executing a fixed instruction set. In a proposed cognitive computer, the kernel serves as the most fundamental unit of interest. These kernels are autonomous, and in aggregation can be multimodal. While each kernel specializes in a distinct modality, they can roughly be broken down into the following groups:

Sensing kernels perceive the world through text, vision, audio, touch, and even custom interfaces to legacy systems.

Learning kernels build models from sensing kernels, constructing the system's understanding of the world.

Motor kernels coordinate movement and attention, guiding sensor kernels to focus on important input data or steering the thinking process in productive directions.

Expression kernels generate outputs like text, images, and sound. Think of them as reverse sensing kernels. They are also capable of sending information back into legacy systems.

Together, these kernels build a cohesive understanding of a dynamic world and the system's place within it, not unlike how your own brain combines multi-modal sensory inputs, motor outputs, and introspection.

A cognitive computer has no files and no applications in the traditional sense. There is only the fluid experience of interacting with the world through kernels that continuously learn and evolve alongside the user. Information exists within the kernels like memories, interconnected and contextual, rather than as isolated items in folders.

To make the transition between radically different paradigms like traditional computer and cognitive computer easier, we must develop a shared language, often using analogies, that helps us grasp the new system in more detail. In the next few sections I will try to explain hypothetical scenarios of how a user might use a cognitive computer and then dedicate a section building parallels between traditional and cognitive computing.

Echoes in the Machine
Imagine a computing experience fundamentally different from managing files or launching siloed apps. Instead, you engage with a system that understands how different parts of your life and experiences connect based on the information you provide.

Consider listening to a piece of instrumental music that evokes a strong, yet indistinct, sense of nostalgia or reminds you of a specific creative period you can't quite place. On a traditional computer, you might struggle to pinpoint the connection, perhaps searching for the track name, then manually searching through notes, documents, or photo folders from roughly when you think you last heard it, hoping to find related items.

With a cognitive computer, the interaction could be profoundly different because there are no separate files to search through. All information—music, images, text, interactions—is part of the system's integrated memory within its kernels. You might ask, "What was I working on when this music was playing last autumn?" or "Show me context around this track from October." The system doesn't search metadata; its kernels access the interconnected memory stream. It correlates the music playing at specific times with other activities or data points experienced or created concurrently. It links the music across your listening history to photos taken during those times, notes written, or even code committed, identifying patterns of co-occurrence within your personal context.

Instead of just listing potentially related files, it might synthesize an insight based on these temporal and contextual connections: "This melody was frequently playing in the background while you were developing the 'autumn landscapes' photo series last October, particularly during your trip to the mountains. You took several pictures of the changing leaves near the waterfall around that time and also wrote a note describing your feeling as 'peaceful and reflective'." It might even generate a small visual collage blending snippets of those photos with colors evoked by the music, helping you reconnect with that specific moment and the creative work associated with it, all inferred from the unified memory within the kernels.

This isn't searching through files; it's the system accessing its integrated memory, leveraging an evolving understanding of your personal history and context—as represented by the interconnected data within the kernels—to facilitate rediscovery, reflection, and creativity in a way current systems cannot. The interaction shifts from giving commands to a passive tool, towards collaborating with a partner that possesses memory, pattern recognition, and the capacity to understand context. This signifies a fundamentally new relationship with technology.

Ghosts in the Architecture
Describing cognitive computing purely on its own terms can be challenging because it departs so significantly from familiar concepts. Interactions change, and the vocabulary is new. To bridge this gap, let's explore how some traditional computing ideas might map onto the cognitive computer's kernel-based architecture, using analogies to aid understanding.

What is a "File"? In traditional systems, a file is a discrete container for data. In a cognitive computer, there are no files in this sense. Information perceived by Sensing kernels (like text from an email, pixels from an image, or audio from a recording) is integrated into models maintained by Learning kernels. What might appear similar to a file (e.g., a document you are reading) is better understood as a persistent view or representation generated by Expression kernels. This view draws dynamically from the underlying, interconnected memory based on your current context and intent, guided by Motor kernels managing attention. It's not a static object you open, but a fluid manifestation of information relevant to the moment.

What is an "Application"? Traditionally, an application is a distinct program designed for specific tasks (word processing, photo editing, email). In a cognitive computer, these rigid boundaries dissolve. Functionality emerges from the coordinated activity of relevant kernels. For example, "editing a photograph" isn't launching a photo app. Instead, your intent (expressed perhaps verbally or through gesture, perceived by Sensing kernels) activates relevant kernels. Learning kernels access the image model, Motor kernels might guide focus ("zoom in here"), other Learning kernels interpret modification requests ("make it brighter"), and Expression kernels generate the updated visual output. The "application" becomes the dynamic constellation of kernels collaborating on your task, seamlessly integrating different modalities (vision, language, action) as needed.

Crucially, this functionality isn't typically "written" or "installed" in the traditional sense. While kernels might possess foundational capabilities, complex behaviors like photo editing or document composition emerge and refine over time. Learning kernels observe user interactions, identifying patterns between intent, kernel activation, and successful outcomes. They learn to orchestrate the Sensing, Motor, and Expression kernels more effectively for recurring tasks. New capabilities can arise from novel combinations of existing kernel functions, guided by Motor kernels exploring solutions or even by direct user guidance shaping the system's responses, akin to teaching rather than programming. This contrasts sharply with traditional software development, offering a more fluid, adaptable, and integrated way to access and evolve system capabilities.

Reading an Email: Instead of opening an email client (an application) to view email messages (files), Sensing kernels perceive incoming communication data. Learning kernels integrate it into your broader communication context, understanding sender, topic, and relevance. Expression kernels present this information, perhaps as a summary or full view, depending on what Motor kernels determine is most relevant to your current focus. The artificial separation between "email app" and other information vanishes.

These parallels are imperfect analogies, of course. The essence of cognitive computing lies in moving beyond these discrete, bounded concepts towards a more fluid, integrated, and context-aware interaction model, much closer to how we naturally process the world.

Learning to See
How does this become possible? Intelligence, both human and artificial, doesn't likely arise from a single monolithic model. It emerges from a distributed network of processes, or kernels, each learning through its own sensory and motor experiences. These kernels build knowledge by doing. They learn not just by passively observing data, but by acting, predicting outcomes, and refining their models based on feedback.

Think of a system learning to draw. Initially, it might move a virtual pen randomly, observing the marks it makes. Gradually, it connects motion to visual results. These associations become richer, allowing it to recognize patterns. When asked to draw a triangle, it experiments, fails, adjusts, and tries again, improving over time. This learning mirrors how humans explore and understand the world. It's not about mimicking human behavior superficially; it's about building upon similar foundational principles of learning through interaction.

In this model, you never explicitly "open an app." You engage with kernels. Need to understand an image? Vision sensing kernels analyze it. Need to articulate a thought? Language expression kernels compose it. Need to compare, recall, or share information? You aren't switching between isolated programs. You're navigating a continuous sensory-motor cognitive loop, orchestrated by learning kernels that manage context and intent. Every interaction becomes perceptual, contextual, agentic, and embodied within the system's evolving understanding. The system isn't just reactive; it explores, predicts, and learns alongside you.

Weaving the Cognitive Fabric
Building such a system requires a different approach. We don't start strictly from the bottom (hardware) or the top (UI). We begin in the middle, where thought meets action, focusing first on how these kernels communicate. We need a flexible protocol, perhaps akin to neurons communicating across synapses, allowing kernels to broadcast, subscribe, and adapt, fostering emergent meaning through shared context.

Next comes the context engine, acting less like a rigid process list and more like working memory, tracking what's relevant, active, and changing based on your current focus. The underlying runtime needs to manage cognitive bandwidth and attention, not just CPU cycles and threads. Only then do we layer on the visible interfaces, shells that reveal the system's thinking and allow collaboration, not just command execution.

Is this feasible now? The necessary components are rapidly aligning. We have powerful hardware like GPUs, NPUs, and emerging neuromorphic chips. We have advancements in sparse representations, sensor fusion, and multimodal AI models. Perhaps most importantly, the mindset is shifting. Users are increasingly ready for digital assistants that can genuinely think and collaborate, not just follow instructions. Critiques of current AI, highlighting the limitations of purely data-driven large models in achieving true understanding or common sense reasoning, underscore the need for architectures like this—ones grounded in interaction, embodiment, and principles closer to biological cognition rather than just statistical pattern matching on vast datasets. And crucially, we have the intricate blueprint of the human brain, the only example of generalized intelligence we know, serving as our north star.

Ultimately, the journey toward truly cognitive computing isn't just about building a better operating system. It's about exploring the potential for a new kind of digital entity – one that lives with us, grows with us, and learns from us in a more natural, symbiotic relationship. It's an invitation to build technology that adapts to us, finally alleviating the human cost of computing and unlocking new frontiers of thought and creativity.












## Grand ideas:

OS without a file system, files and documents.

Computer is a brain made out of modules that can sense, learn and act.

There are no documents and no apps. Apps - are skills, documents are knowledge. Any external interaction is interaction with this knowledge and is done via sensing, learning and actions.



## The Human Cost of Computing

I admire how good we are at adapting. We've come a long way. From hunter-gatherers to modern techno-sapiens, we went through many irreversable cognitive transformations. The last one happened in a blink of an eye. In the past 30 years computers rewired us. They gave us tremendous powers, but took away our patience. They gave us freedom in communication but took away our atonomy. We were forced to schedule our lives around software updates and computer restarts, learn how to navigate file systems, memorize obscure keyboard shortcuts, while bending our fingers in unnatural gymnastics, and while teaching us all these tricks they helped us develop selective blindness to the ever-growing unread email count in our inboxes. In the end we were chained to the tiny screens in our pockets. 

When I speak of computers, of course, I speak not only of traditional desktop machines and laptops, but of an entire ecosystem of devices: smartphones, tablets, TVs, watches, thermostats, and countless others. And what unites these is how collectively they've impacted our thinking, our actions, and our habits. Not always for the better.

Many talented designers and engineers today work tirelessly on how to make interactions with these devices more natural and less painful. But being held back by technological convention, it's a hard task to get right. 

Making something radically different has tremendous friction. For example, a world without files and folders feels unsettling. Changing a QWERTY staggered keyboard warrants public rejection, even though the encumbant is neither efficient nor ergonomic way to enter text. 

This reveals just how deeply we've internalized historical conventions that were unnatural but neccessary.   

## A New Computing Paradigm

Most people underestimate how radically better human-centered computing can be, just as they underestimate how deeply entrenched is our current condition. 

Let's pretend for a moment that computer is a person, or even a friend.

Communication bandwidth between friends is greater than between strangers. You understand each other faster. You receive help before asking. Your shared world has big contextual overlap. You live in the same universe of common sense knowledge. That's the relationship we aim for between a person and a computers.

In order to achieve that level of interaction we first must understand what makes these interaction between people and between a person and the world possible.

At the center of this is the human brain and in particular the neocortex. 

Envisioning an operating system based on these principles, we anticipate a paradigm shift away from files, folders, and apps, toward a brain-like assistant that organizes and interacts with data in a human-like way. This would revolutionize how users experience computing. 

Traditional OSes revolve around files (documents, images, etc.) stored in hierarchical folders. In a brain-inspired OS, the fundamental unit would be an **object** or concept, learned and represented distributively. Instead of a file sitting inert in a folder, an “object” in this OS could be a composite of all information the system knows about a thing. For example, “Project Alpha” might exist as an object in the system – it would encompass relevant emails, documents, people, timelines, even images associated with the project. The system, having ingested all those through its sensors (text, vision, etc.), links them in the object’s SDR representation. When you want to retrieve or work on something, you wouldn’t navigate a directory tree; you’d simply call up the object by name or description. Because the OS understands semantic similarity, you could also search in a very human way: *“find that graph I made for the project that compared last year’s sales”* – and the system’s vision modules and text modules together would recall the specific graph (even if it’s embedded in a slide deck somewhere) because it knows the object “Project Alpha” and the concept “sales graph.” This is a radical shift from current desktop search, which relies on file names or keywords. It moves toward the idea of a **semantic memory** for your computer.

With the system modeling data in a human-like fashion, interacting with it becomes much more natural. The user experience would be akin to working with a smart assistant or even an extension of one’s mind, rather than operating software.

- **Multimodal Conversations:** You could communicate with the OS through voice, text, gestures, or even by showing it things.
  
- **Continuous Context Awareness:** The brain-inspired OS, via its always-on sensory processing, can maintain context.

- **Learning from the User:** The more the user interacts, the more the OS learns their preferences, routines, and vocabulary. If you often say “send a summary to the team,” the OS might learn what level of detail “summary” implies, who “the team” is, and which channel to send it (email, chat, etc.), without being explicitly programmed.

This kind of *cross-application context sharing* is possible because under the hood there are no rigid app boundaries – everything funnels into the unified cortical memory. The result is an assistant-like behavior: the OS anticipates what you might need based on context, much as a human assistant would note, “Hey, the client just asked about X, and I recall you have a document on X open.” This makes interaction fluid and reduces the need for the user to manually juggle applications.

Crucially, because the system is continuously learning and updating its models, it doesn’t stagnate; it adapts as your life and work change. Within a few years of daily use, such an OS might develop a rich model of your personal “ontology,” enabling very powerful queries like *“Have I ever met this person before?”* or *“Show me references in my notes that relate to this new idea”* – and get meaningful answers.

User Experience Grounded in Object-Based Cognition 
Moving away from “apps” could be the most challenging paradigm shift. Today, we open a specific application to perform a task (Word for writing, Chrome for web, etc.). In an object-centric OS, the focus is instead on the *content or task*, not the tool. The system might present a unified workspace for a given object or project, which gathers all relevant functionality automatically. For example, if you’re working on a “Budget Report” object, the OS knows it involves numbers, text, and charts. It could provide an editing interface that has spreadsheet-like and document-like capabilities in one view, instead of you switching between Excel and Word. The distinction between applications blurs because the intelligence of the system can bring the right operations to you based on context. This is analogous to how our brain doesn’t switch “apps” – we seamlessly shift modes (imagine reading a document, then calculating something in our head, then drawing a diagram on paper; to the brain it’s all one cognitive workspace). Achieving this in software might involve modular tools that the OS can deploy as needed behind the scenes.

**Common Sense Services:** With a robust world model, the OS can provide what we might call *common sense services* to the user.


Humans naturally understand the world through contextualized events in time. If I ask you where something is in your house, you'll have to mentally walk to that place to answer the question. If I ask you to sing a melody you can't help but will sing it from the start. You physically can't do a lot of things without first going to the very beginning. Most things we know are moments arranged linearly in time, from the beginning to point where information we're searching for is stored. The path from the beginning to that point is context. Even most basic things like objects, materials and textures are grounded in a sequences of tactile responses combined with sequence of proprioception events. So are thoughts and ideas.

Everything that enters and exits your computer should be automatically contextualized. When you need to reference information, you don't search by filename or location. You find it through natural connections, just like you would with a real human. Context becomes the primary organizing principle, mirroring how your mind actually works.

Sequences of events is the file system. Everything else builds on top of that. 

The artificial boundary between documents and programs should not exist. Today, we treat documents as containers and programs as systems. There should not be a destinction between the two.

Operating system itself is built and changed by the user. Users can mold their operating system at multiple levels of abstraction. Advanced users can work directly with low-level components, while less technical users can build experiences from pre-made building blocks.


## Building the Future

I don't claim to have all the answers. I can be wrong about details or even the entire approach. No one has the answers until we try.

Our past cannot be changed. However we can plot our future in whatever direction we want. This isn't about demolishing everything that came before, but creating space for a new computing paradigm — one that's simpler, more intuitive, and fundamentally aligned with human thought processes rather than machine architectures.

First, we need a solid foundation for context-aware computing. This means developing systems that understand relationships between different pieces of information, not just their content. A photo isn't just pixels—it's an event, people, a location, emotions, and connections to other moments.

Next, we design the spectrum of control that allows users at different technical levels to mold their system to their needs. This enables technological freedom. We must have intuitive high-level building blocks for anyone to understand and combine, and low-level building blocks for more technical users to work at a comfortable levels of abstraction. The space between low and high levels is a gradient. Users can "zoom in" on complexity as their skills evolve or "zoom out" in search of simplisity.  




We systematically deconstruct existing abstractions, evaluating each for its actual utility to humans versus its historical technical necessity. File systems, application boundaries, user accounts—all must be reimagined based on how humans naturally think and work.

Computers must become more human. Not the other way around.

The distinction between documents and programs becomes one of the first boundaries to fall. When I read a text document, why can't I talk to it directly? What if the text document had the whole history, the thought process embedded and interactive in it. When I create a program, why can't I annotate it like a document? Everything should become a blend of content and capability, with proportions shifting based on context rather than rigid type definitions.

Large language models serve as the translation layer between human intent and system capabilities. Rather than forcing humans to learn machine languages, we teach machines to understand human languages and contexts. This creates a dialogue where both sides contribute their strengths: humans provide intuition and goals; machines provide precision and processing power.







By answering fundamental questions about what we actually need from modern computing, we can work backward to describe an ideal system at interaction, software, OS, and hardware levels. What capabilities truly matter? What cognitive burdens can be eliminated? What possibilities emerge when we shed historical constraints?

This won't be easy. There will be intrinsic complexity and human resistance. Some things are inherently unpredictable. Many existing structures are inefficient but difficult to change. By questioning our fundamental assumptions about computing, we open the door to systems that are not just incrementally better, but transformatively more aligned with human needs and capabilities. A more natural world.





----



Modern hardware is billions of times faster than what was available 30 years ago, yet it's not billions of times more useful. The faster hardware grew, the more convoluted software became - a compressed evolution of computing power without a corresponding revolution in human-computer interaction.

File systems are archaic, divorced from human thought patterns. Your mind doesn't store memories in folders. You recall a vacation through emotions felt, people present, weather that day—through context. Yet we've forced ourselves to adapt to rigid, mechanistic storage systems because that's what computers could handle decades ago.


Most people underestimate how radical the upside of reimagining computing could be, just as they underestimate how deeply entrenched our current paradigms have become. The only thing standing between us and a fundamentally better computing experience is our willingness to question established norms.


Our communication has grown more structured, our problem-solving more algorithmic, and our attention spans calibrated to notification cycles. The democratization of computing access wasn't achieved by making computers truly intuitive to the human mind, but by slowly reshaping humanity to better interface with our digital creations. We didn't build technology that truly understands us; we rebuilt ourselves to understand it.



Computing progressed through increasingly abstract layers of 1s and 0s, assembly language, compilers, high-level programming languages, operating systems, application frameworks, graphical interfaces, and user experience design. We've built layers upon layers in a race to take advantege of ever-growing computing power, never pausing to reevaluate how we design. 












When you receive a photo or document, its meaning comes from its relationships: who sent it, what it contains, how it relates to your other information. We must recognize that context is the fundamental organizing principle of human thought.



Heavy computation can happen remotely, freeing your local device from unnecessary constraints. The artificial boundary between "local" and "cloud" disappears, creating a unified experience that leverages the best of both worlds. Your experience remains consistent while complex processing happens invisibly in the background. 







